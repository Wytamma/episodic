{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Episodic","text":"<p>A complete pipeline for fitting and testing Fixed Local Clock (FLC) molecular clock models for episodic evolution.</p> <p> </p>"},{"location":"#about","title":"About","text":"<p>Episodic is a tool for fitting and testing Fixed Local Clock (FLC) molecular clock models for episodic evolution. The package is built on top of SNK, and provides a complete pipeline for fitting and testing models of episodic evolution using BEAST.</p> <p>Episodic implements the ideas of Tay et al. (2022 and 2023) and detects episodic evolution through Bayesian inference of molecular clock models. </p> <p>Given a multiple sequence alignment and a list of groups to test for episodic evolution, episodic will: - Configure BEAST analyses for strict, relaxed (UCGD) and stem fixed local clock models.  - Configure marginal likelihood analyses for each clock model. - Run all the BEAST and marginal likelihood analyses. - Plot and summarise the results. - Compute and plot Bayes factors for the marginal likelihood analyses. - Produce maximum clade credibility (MCC) trees for each clock model. - Compute bayes factor on effect size for the FLC models (foreground vs background). - Run rank and quantile tests on the all the models. - Handel the execution of the pipeline on a HPC cluster via snakemake profiles. - Produce a report of the results (TBD).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Complete pipeline - <code>episodic</code> provides a complete pipeline for fitting and testing FLC models of episodic evolution.</li> <li>Flexible - <code>episodic</code> is built on top of SNK, and provides a flexible framework for fitting and testing FLC models of episodic evolution.</li> <li>Easy to use - <code>episodic</code> is easy to use, and provides a simple interface for fitting and testing FLC models of episodic evolution.</li> <li>robust - <code>episodic</code> is robust, and provides a robust framework for fitting and testing FLC models of episodic evolution. </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>The episodic package can be installed via pip.</p> <pre><code>pip install episodic\n</code></pre> <p>Episodic requires conda to be installed. If you do not have conda installed, you can install it via the miniconda package.</p> <p>Once installed run the following command to install the required conda environments.</p> <pre><code>episodic env create\n</code></pre> <p>This will download and install the required conda environments for the pipeline. This includes packages like BEAST, python, R, etc. </p>"},{"location":"quickstart/","title":"QuickStart","text":"<p>Before running the pipeline it is sujjested that your read Tay et al. (2022 and 2023) and familiarise yourself with the concepts of Fixed Local Clock (FLC) models.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>The episodic package can be installed via pip.</p> <pre><code>pip install episodic\n</code></pre>"},{"location":"quickstart/#running-the-pipeline","title":"Running the pipeline","text":"<p>The pipeline can be run using the <code>episodic</code> command line interface. </p> <pre><code>episodic run --fasta tests/data/.yaml\n</code></pre>"},{"location":"quickstart/#confuguration","title":"Confuguration","text":"<p>The pipeline requires a configuration file to be provided. The configuration file is a YAML file that specifies the input data and the groups to test for episodic evolution.</p>"},{"location":"reference/scripts/","title":"Scripts","text":""},{"location":"reference/scripts/#src.episodic.workflow.utils.date_to_decimal_year","title":"<code>date_to_decimal_year(date_str)</code>","text":"<p>Converts a date in the format '%Y-%m-%d' to a decimal year.</p> <p>Parameters:</p> Name Type Description Default <code>date_str</code> <code>str</code> <p>The date in the format '%Y-%m-%d'.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The decimal year.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; date_to_decimal_year('2020-07-02')\n2020.5\n</code></pre> Source code in <code>src/episodic/workflow/utils.py</code> <pre><code>def date_to_decimal_year(date_str):\n    \"\"\"\n    Converts a date in the format '%Y-%m-%d' to a decimal year.\n\n    Args:\n      date_str (str): The date in the format '%Y-%m-%d'.\n\n    Returns:\n      float: The decimal year.\n\n    Examples:\n      &gt;&gt;&gt; date_to_decimal_year('2020-07-02')\n      2020.5\n    \"\"\"\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    year = date.year\n    start_of_year = datetime(year, 1, 1)\n    end_of_year = datetime(year + 1, 1, 1)\n    days_in_year = (end_of_year - start_of_year).days\n    days_passed = (date - start_of_year).days\n    decimal_year = year + days_passed / days_in_year\n    return decimal_year\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.utils.decimal_year_to_date","title":"<code>decimal_year_to_date(decimal_year)</code>","text":"<p>Converts a decimal year to a date in the format '%Y-%m-%d'.</p> <p>Parameters:</p> Name Type Description Default <code>decimal_year</code> <code>float</code> <p>The decimal year to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The date in the format '%Y-%m-%d'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decimal_year_to_date(2020.5)\n'2020-07-02'\n</code></pre> Source code in <code>src/episodic/workflow/utils.py</code> <pre><code>def decimal_year_to_date(decimal_year):\n    \"\"\"\n    Converts a decimal year to a date in the format '%Y-%m-%d'.\n\n    Args:\n      decimal_year (float): The decimal year to convert.\n\n    Returns:\n      str: The date in the format '%Y-%m-%d'.\n\n    Examples:\n      &gt;&gt;&gt; decimal_year_to_date(2020.5)\n      '2020-07-02'\n    \"\"\"\n    year = int(decimal_year)\n    remainder = decimal_year - year\n    start_of_year = datetime(year, 1, 1)\n    end_of_year = datetime(year + 1, 1, 1)\n    days_in_year = (end_of_year - start_of_year).days\n    days = remainder * days_in_year\n    date = start_of_year + timedelta(days=days)\n    return date.strftime('%Y-%m-%d')\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.phylo_rate_quantile_analysis.analyze_rates","title":"<code>analyze_rates(trees_path=typer.Argument(..., help='Path to the BEAST output trees file'), groups=typer.Option(..., '--group', '-g', help='Group labels to analyze'), output_plot_path=typer.Option(..., '--output-plot', help='Output path for the plot file'), output_csv_path=typer.Option(..., '--output-csv', help='Output path for the CSV file'), burnin=typer.Option(0.1, '--burnin', '-b', help='Fraction of trees to discard as burn-in'))</code>","text":"<p>Analyzes rates from a given BEAST output trees file and generates a plot and CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>trees_path</code> <code>str</code> <p>The path to the BEAST output trees file.</p> <code>typer.Argument(..., help='Path to the BEAST output trees file')</code> <code>groups</code> <code>List[str]</code> <p>The group labels to analyze.</p> <code>typer.Option(..., '--group', '-g', help='Group labels to analyze')</code> <code>output_plot_path</code> <code>str</code> <p>The output path for the plot file.</p> <code>typer.Option(..., '--output-plot', help='Output path for the plot file')</code> <code>output_csv_path</code> <code>str</code> <p>The output path for the CSV file.</p> <code>typer.Option(..., '--output-csv', help='Output path for the CSV file')</code> <code>burnin</code> <code>float</code> <p>The fraction of trees to discard as burn-in.</p> <code>typer.Option(0.1, '--burnin', '-b', help='Fraction of trees to discard as burn-in')</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; analyze_rates('trees.nexus', ['A', 'B'], 'plot.png', 'stats.csv', 0.1)\n</code></pre> Source code in <code>src/episodic/workflow/scripts/phylo_rate_quantile_analysis.py</code> <pre><code>@app.command()\ndef analyze_rates(\n    trees_path: str = typer.Argument(..., help=\"Path to the BEAST output trees file\"),\n    groups: List[str] = typer.Option(..., \"--group\", \"-g\", help=\"Group labels to analyze\"),\n    output_plot_path: str = typer.Option(..., \"--output-plot\", help=\"Output path for the plot file\"),\n    output_csv_path: str = typer.Option(..., \"--output-csv\", help=\"Output path for the CSV file\"),\n    burnin: float = typer.Option(0.1, \"--burnin\", \"-b\", help=\"Fraction of trees to discard as burn-in\"),\n):\n    \"\"\"\n    Analyzes rates from a given BEAST output trees file and generates a plot and CSV file.\n\n    Args:\n      trees_path (str): The path to the BEAST output trees file.\n      groups (List[str]): The group labels to analyze.\n      output_plot_path (str): The output path for the plot file.\n      output_csv_path (str): The output path for the CSV file.\n      burnin (float): The fraction of trees to discard as burn-in.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; analyze_rates('trees.nexus', ['A', 'B'], 'plot.png', 'stats.csv', 0.1)\n    \"\"\"\n    # time ow long it takes to run\n    now = datetime.now()\n    tree_yielder = dendropy.Tree.yield_from_files(files=[trees_path], schema=\"nexus\", preserve_underscores=True)\n    total_trees = sum(1 for _ in tree_yielder)  # Count total trees\n    burnin_count = int(total_trees * burnin)\n    total_time = datetime.now() - now\n    print(f\"Total time to count trees: {total_time}\")\n\n    tree_yielder = dendropy.Tree.yield_from_files(  # Reinitialize generator\n        files=[trees_path], schema=\"nexus\", preserve_underscores=True\n    )\n    group_stats: Dict[str, Dict[str, List]] = {g: {\"ranks\": [], \"quantiles\": []} for g in groups}\n\n    with typer.progressbar(\n            tree_yielder,\n            length=total_trees,\n            label=\"Processing trees\",\n            show_pos=True,\n            show_percent=True\n        ) as progress:\n        for tree_idx, tree in enumerate(progress):\n            if tree_idx &lt; burnin_count:\n                continue\n            analyze_tree(tree, groups, group_stats)\n\n    csv_data = [\n        [\n            \"Group\",\n            \"Mean Rank\",\n            \"Rank Credible Interval\",\n            \"Mean Quantile\",\n            \"Quantile Credible Interval\",\n        ]\n    ]\n\n    plt.figure(figsize=(15, 5 * len(groups)))\n\n    for i, group in enumerate(groups, start=1):\n        ranks = group_stats[group][\"ranks\"]\n        quantiles = group_stats[group][\"quantiles\"]\n\n        mean_rank = np.mean(ranks)\n        rank_credible_interval = (np.percentile(ranks, 2.5), np.percentile(ranks, 97.5))\n\n        mean_quantile = np.mean(quantiles)\n        quantile_credible_interval = (\n            np.percentile(quantiles, 2.5),\n            np.percentile(quantiles, 97.5),\n        )\n\n        csv_data.append(\n            [\n                group,\n                f\"{mean_rank:.4f}\",\n                f\"[{rank_credible_interval[0]:.0f}, {rank_credible_interval[1]:.0f}]\",\n                f\"{mean_quantile:.4f}\",\n                f\"[{quantile_credible_interval[0]:.2f}, {quantile_credible_interval[1]:.2f}]\",\n            ]\n        )\n\n        plt.subplot(len(groups), 2, 2 * i - 1)\n        plt.hist(ranks, bins=30, alpha=0.7, color=\"blue\")\n        rank_credible_interval_str = f\"[{rank_credible_interval[0]:.0f}, {rank_credible_interval[1]:.0f}]\"\n        plt.title(f\"Ranks for {group} - Mean: {mean_rank:.2f}, 95% CI: {rank_credible_interval_str}\")\n        plt.xlabel(\"Rank\")\n        plt.ylabel(\"Frequency\")\n\n        plt.subplot(len(groups), 2, 2 * i)\n        plt.hist(quantiles, bins=30, alpha=0.7, color=\"green\")\n        quantile_credible_interval_str = f\"[{quantile_credible_interval[0]:.2f}, {quantile_credible_interval[1]:.2f}]\"\n        plt.title(f\"Quantiles for {group} - Mean: {mean_quantile:.2f}, 95% CI: {quantile_credible_interval_str}\")\n        plt.xlabel(\"Quantile\")\n        plt.ylabel(\"Frequency\")\n\n    plt.tight_layout()\n\n    plt.savefig(output_plot_path)\n\n    with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerows(csv_data)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.phylo_rate_quantile_analysis.analyze_tree","title":"<code>analyze_tree(tree, groups, group_stats)</code>","text":"<p>Analyzes a given tree and updates group statistics.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dendropy.Tree</code> <p>The tree to analyze.</p> required <code>groups</code> <code>List[str]</code> <p>The group labels to analyze.</p> required <code>group_stats</code> <code>Dict[str, Dict[str, List]]</code> <p>A dictionary containing group statistics.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; analyze_tree(tree, ['A', 'B'], {'A': {'ranks': [], 'quantiles': []}, 'B': {'ranks': [], 'quantiles': []}})\n</code></pre> Source code in <code>src/episodic/workflow/scripts/phylo_rate_quantile_analysis.py</code> <pre><code>def analyze_tree(tree, groups, group_stats):\n    \"\"\"\n    Analyzes a given tree and updates group statistics.\n\n    Args:\n      tree (dendropy.Tree): The tree to analyze.\n      groups (List[str]): The group labels to analyze.\n      group_stats (Dict[str, Dict[str, List]]): A dictionary containing group statistics.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; analyze_tree(tree, ['A', 'B'], {'A': {'ranks': [], 'quantiles': []}, 'B': {'ranks': [], 'quantiles': []}})\n    \"\"\"\n    # Assuming sorted_rates is generated here for each tree passed to this function\n    sorted_rates = extract_and_sort_rates(tree)\n    rates = np.array(sorted_rates)  # For efficient operations with numpy\n    for group in groups:\n        nodes_in_clade = [node for node in tree if node.taxon is not None and group in node.taxon.label]\n        mrca = tree.mrca(taxon_labels=[node.taxon.label for node in nodes_in_clade])\n        group_rate = float(mrca.annotations.get_value(\"rate\"))\n\n        # Use bisect_left for efficient rank finding in a sorted list\n        rank = bisect_left(sorted_rates, group_rate) + 1\n        group_stats[group][\"ranks\"].append(rank)\n\n        # Use numpy for efficient percentile calculation\n        quantile = np.percentile(rates, group_rate, method=\"lower\")\n        group_stats[group][\"quantiles\"].append(quantile)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.phylo_rate_quantile_analysis.extract_and_sort_rates","title":"<code>extract_and_sort_rates(tree)</code>","text":"<p>Extracts and sorts rates from a given tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dendropy.Tree</code> <p>The tree to extract rates from.</p> required <p>Returns:</p> Type Description <p>List[float]: A list of sorted rates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_and_sort_rates(tree)\n[0.1, 0.2, 0.3]\n</code></pre> Source code in <code>src/episodic/workflow/scripts/phylo_rate_quantile_analysis.py</code> <pre><code>def extract_and_sort_rates(tree):\n    \"\"\"\n    Extracts and sorts rates from a given tree.\n\n    Args:\n      tree (dendropy.Tree): The tree to extract rates from.\n\n    Returns:\n      List[float]: A list of sorted rates.\n\n    Examples:\n      &gt;&gt;&gt; extract_and_sort_rates(tree)\n      [0.1, 0.2, 0.3]\n    \"\"\"\n    rates = [\n        float(node.annotations.get_value(\"rate\"))\n        for node in tree\n        if node.annotations.get_value(\"rate\")\n    ]\n    sorted_rates = sorted(rates)\n    return sorted_rates\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.arviz_output.load_log_files","title":"<code>load_log_files(logs, burnin=0.1)</code>","text":"<p>Loads BEAST log files into a single pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>List[Path]</code> <p>A list of paths to the log files.</p> required <code>burnin</code> <code>float</code> <p>The fraction of the chain to discard as burnin.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: A DataFrame containing the log data.</p> Source code in <code>src/episodic/workflow/scripts/arviz_output.py</code> <pre><code>def load_log_files(logs: List[Path], burnin: float = 0.1) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads BEAST log files into a single pandas DataFrame.\n\n    Args:\n      logs (List[Path]): A list of paths to the log files.\n      burnin (float): The fraction of the chain to discard as burnin.\n\n    Returns:\n      pd.DataFrame: A DataFrame containing the log data.\n    \"\"\"\n    model_groups = defaultdict(list)\n    for path in logs:\n        model = path.parent.parent.name # assumes that the model name is the parent of the parent of the log file\n        model_groups[model].append(path)\n\n    dfs = []\n    for model, paths in model_groups.items():\n        model_df = pd.DataFrame()\n        chain_count = 0\n        for trace_log in paths:\n            print(trace_log)\n            duplicate_df = pd.read_csv(trace_log, sep=\"\\t\", comment=\"#\").rename(columns={\"state\": \"draw\"})\n            posterior_df = duplicate_df.truncate(before=burnin * len(duplicate_df))\n            posterior_df[\"chain\"] = chain_count\n            model_df = pd.concat([model_df, posterior_df])\n            chain_count += 1\n        if len(model_groups) &gt; 1:\n            # rename the columns to include the model name when there are multiple models\n            var_names = [c for c in model_df.columns if c not in (\"chain\", \"draw\")]\n            rename_dict = {col: f\"{model}.{col}\" for col in var_names}\n            model_df.rename(columns=rename_dict, inplace=True)\n        if len(dfs):\n            # drop the chain and draw columns from all but the first model\n            model_df.drop(columns=[\"chain\", \"draw\"], inplace=True)\n        dfs.append(model_df)\n    df = pd.concat(dfs, axis=1)\n    df = df.set_index([\"chain\", \"draw\"])\n    return df\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.arviz_output.no_browser","title":"<code>no_browser()</code>","text":"<p>A context manager that temporarily replaces the Bokeh show function with a dummy one.</p> <p>Yields:</p> Name Type Description <code>None</code> <p>Allows the code inside the 'with' block to run.</p> Notes <p>This context manager is used to prevent the Bokeh show function from opening a browser window when saving a plot.</p> Source code in <code>src/episodic/workflow/scripts/arviz_output.py</code> <pre><code>@contextmanager\ndef no_browser():\n    \"\"\"\n    A context manager that temporarily replaces the Bokeh show function with a dummy one.\n\n    Yields:\n      None: Allows the code inside the 'with' block to run.\n\n    Notes:\n      This context manager is used to prevent the Bokeh show function from opening a browser window when saving a plot.\n    \"\"\"\n    # Save the original show function\n    original_show = bokeh.io.showing._show_file_with_state\n\n    # Define a custom show function that does nothing\n    def dummy_show(obj, state, *args, **kwargs):\n        \"\"\"\n    A custom show function that does nothing.\n\n    Args:\n      obj (object): The object to show.\n      state (object): The state of the object.\n      *args: Additional positional arguments.\n      **kwargs: Additional keyword arguments.\n\n    Returns:\n      None: Does not return anything.\n    \"\"\"\n        filename = save(obj, state=state)\n\n    # Replace the Bokeh show function with the dummy one\n    bokeh.io.showing._show_file_with_state = dummy_show\n\n    try:\n        yield  # This allows the code inside the 'with' block to run\n    finally:\n        # Restore the original show function\n        bokeh.io.showing._show_file_with_state = original_show\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.arviz_output.rates","title":"<code>rates(logs=typer.Argument(..., help='BEAST log files'), output_prefix=typer.Option(..., help='Prefix for output files'), gamma_shape=typer.Option(..., help='Shape parameter for the gamma prior'), gamma_scale=typer.Option(..., help='Scale parameter for the gamma prior'), burnin=typer.Option(0.1, help='Fraction of the chain to discard as burnin'))</code>","text":"<p>Plots the rates from BEAST log files.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>List[Path]</code> <p>A list of paths to the log files.</p> <code>typer.Argument(..., help='BEAST log files')</code> <code>output_prefix</code> <code>Path</code> <p>The prefix for the output files.</p> <code>typer.Option(..., help='Prefix for output files')</code> <code>gamma_shape</code> <code>float</code> <p>The shape parameter for the gamma prior.</p> <code>typer.Option(..., help='Shape parameter for the gamma prior')</code> <code>gamma_scale</code> <code>float</code> <p>The scale parameter for the gamma prior.</p> <code>typer.Option(..., help='Scale parameter for the gamma prior')</code> <code>burnin</code> <code>float</code> <p>The fraction of the chain to discard as burnin.</p> <code>typer.Option(0.1, help='Fraction of the chain to discard as burnin')</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Does not return anything.</p> Source code in <code>src/episodic/workflow/scripts/arviz_output.py</code> <pre><code>@app.command()\ndef rates(\n    logs: List[Path] = typer.Argument(..., help=\"BEAST log files\"),\n    output_prefix: Path = typer.Option(..., help=\"Prefix for output files\"),\n    gamma_shape: float = typer.Option(..., help=\"Shape parameter for the gamma prior\"),\n    gamma_scale: float = typer.Option(..., help=\"Scale parameter for the gamma prior\"),\n    burnin: float = typer.Option(0.1, help=\"Fraction of the chain to discard as burnin\"),\n):\n    \"\"\"\n    Plots the rates from BEAST log files.\n\n    Args:\n      logs (List[Path]): A list of paths to the log files.\n      output_prefix (Path): The prefix for the output files.\n      gamma_shape (float): The shape parameter for the gamma prior.\n      gamma_scale (float): The scale parameter for the gamma prior.\n      burnin (float): The fraction of the chain to discard as burnin.\n\n    Returns:\n      None: Does not return anything.\n    \"\"\"\n    df = load_log_files(logs, burnin=burnin)\n\n    # extract the rate columns\n    var_names = [c for c in df.columns if c.endswith(\".rate\") or c.endswith(\".ucgd.mean\")]\n    df = df[var_names]\n\n\n    # add a prior rate column\n    prior_rate = np.random.gamma(gamma_shape, gamma_scale, len(df))\n    df[\"prior.rate\"] = prior_rate\n\n    # convert to xarray\n    xdata = xr.Dataset.from_dataframe(df)\n    dataset = az.InferenceData(posterior=xdata)\n\n    for rug in (True, False):\n        rug_str = \"violin-rug\" if rug else \"violin\"\n        # plot the rates\n        axs = az.plot_violin(\n            dataset,\n            figsize=(len(df.columns) * 5, 12),\n            textsize=16,\n            sharey=True,\n            sharex=False,\n            rug=rug,\n            grid=(1, len(df.columns)),\n        )\n        plt.savefig(f\"{output_prefix}-{rug_str}.svg\")\n\n        selected_columns = df.columns[~df.columns.isin([\"prior.rate\", \"draw\", \"chain\"])]\n\n        for ax in axs.flatten():\n            ymax = df[selected_columns].max().max() + df[selected_columns].min().std()\n            ymin = df[selected_columns].min().min() - df[selected_columns].min().std()\n            ax.set_ylim(ymin, ymax)\n\n        plt.savefig(f\"{output_prefix}-{rug_str}-trimmed.svg\")\n\n    # plot the trace\n    az.plot_trace(\n        dataset,\n        figsize=(12, len(df.columns) * 4),\n        )\n    plt.savefig(f\"{output_prefix}-trace.svg\")\n\n    # plot interactive trace\n    output_file(filename=f\"{output_prefix}-trace.html\")\n    with no_browser():\n        # hack to save the html file without opening it in the browser\n        # must set show=True to save the file\n        az.plot_trace(dataset, backend=\"bokeh\", show=True)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.arviz_output.summary","title":"<code>summary(logs=typer.Argument(..., help='BEAST log files'), output=typer.Argument(..., help='Output csv file'), burnin=0.1)</code>","text":"<p>Generates a summary of the BEAST log files and saves it to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>List[Path]</code> <p>A list of paths to the log files.</p> <code>typer.Argument(..., help='BEAST log files')</code> <code>output</code> <code>Path</code> <p>The output csv file.</p> <code>typer.Argument(..., help='Output csv file')</code> <code>burnin</code> <code>float</code> <p>The fraction of the chain to discard as burnin.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Does not return anything.</p> Source code in <code>src/episodic/workflow/scripts/arviz_output.py</code> <pre><code>@app.command()\ndef summary(\n        logs: List[Path] = typer.Argument(..., help=\"BEAST log files\"),\n        output: Path = typer.Argument(..., help=\"Output csv file\"),\n        burnin: float = 0.1,\n    ):\n    \"\"\"\n    Generates a summary of the BEAST log files and saves it to a csv file.\n\n    Args:\n      logs (List[Path]): A list of paths to the log files.\n      output (Path): The output csv file.\n      burnin (float): The fraction of the chain to discard as burnin.\n\n    Returns:\n      None: Does not return anything.\n    \"\"\"\n\n    df = load_log_files(logs, burnin=burnin)\n\n    xdata = xr.Dataset.from_dataframe(df)\n    dataset = az.InferenceData(posterior=xdata)\n    summary = az.summary(dataset, round_to=6)\n\n    # save the summary to csv\n    summary.to_csv(output)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.arviz_output.trace","title":"<code>trace(logs=typer.Argument(..., help='BEAST log files'), directory=typer.Argument(..., help='Output directory'), burnin=typer.Option(0.1, help='Fraction of the chain to discard as burnin'))</code>","text":"<p>Plots the trace from BEAST log files.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>List[Path]</code> <p>A list of paths to the log files.</p> <code>typer.Argument(..., help='BEAST log files')</code> <code>directory</code> <code>Path</code> <p>The output directory.</p> <code>typer.Argument(..., help='Output directory')</code> <code>burnin</code> <code>float</code> <p>The fraction of the chain to discard as burnin.</p> <code>typer.Option(0.1, help='Fraction of the chain to discard as burnin')</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Does not return anything.</p> Source code in <code>src/episodic/workflow/scripts/arviz_output.py</code> <pre><code>@app.command()\ndef trace(\n        logs: List[Path] = typer.Argument(..., help=\"BEAST log files\"),\n        directory: Path = typer.Argument(..., help=\"Output directory\"),\n        burnin: float = typer.Option(0.1, help=\"Fraction of the chain to discard as burnin\"),\n):\n    \"\"\"\n    Plots the trace from BEAST log files.\n\n    Args:\n      logs (List[Path]): A list of paths to the log files.\n      directory (Path): The output directory.\n      burnin (float): The fraction of the chain to discard as burnin.\n\n    Returns:\n      None: Does not return anything.\n    \"\"\"\n    df = load_log_files(logs, burnin=burnin)\n\n    # convert to xarray\n    xdata = xr.Dataset.from_dataframe(df)\n    dataset = az.InferenceData(posterior=xdata)\n\n    output_file(filename=directory / f\"{directory.name}-trace.html\", title=\"Static HTML file\")\n\n    with no_browser():\n        # hack to save the html file without opening it in the browser\n        # must set show=True to save the file\n        with az.rc_context(rc={'plot.max_subplots': None}):\n            az.plot_trace(dataset, backend=\"bokeh\", show=True)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.calculate_odds.calculate_log_diff","title":"<code>calculate_log_diff(pos_odds, p_odds)</code>","text":"<p>Calculates the log difference of two odds ratios.</p> <p>Parameters:</p> Name Type Description Default <code>pos_odds</code> <code>float</code> <p>The posterior odds ratio.</p> required <code>p_odds</code> <code>float</code> <p>The prior odds ratio.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The log difference of the two odds ratios.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculate_log_diff(2, 1)\n0.6931471805599453\n</code></pre> Source code in <code>src/episodic/workflow/scripts/calculate_odds.py</code> <pre><code>def calculate_log_diff(pos_odds, p_odds):\n    \"\"\"\n    Calculates the log difference of two odds ratios.\n\n    Args:\n      pos_odds (float): The posterior odds ratio.\n      p_odds (float): The prior odds ratio.\n\n    Returns:\n      float: The log difference of the two odds ratios.\n\n    Examples:\n      &gt;&gt;&gt; calculate_log_diff(2, 1)\n      0.6931471805599453\n    \"\"\"\n    log_pos_odds = safe_log(pos_odds)\n    log_p_odds = safe_log(p_odds)\n\n    if log_pos_odds is not None and log_p_odds is not None:\n        log_diff = log_pos_odds - log_p_odds\n    else:\n        # Handle the case where one or both of the odds are not valid for log operation\n        log_diff = float(\"inf\")  # Or choose another appropriate response\n\n    return log_diff\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.calculate_odds.calculate_odds","title":"<code>calculate_odds(logs=typer.Argument(..., help='The path to the log CSV file'), output_file=typer.Argument(..., help='The path to the output CSV file where the results will be saved'), gamma_shape=typer.Option(..., help='The shape parameter for the gamma distribution'), gamma_scale=typer.Option(..., help='The scale parameter for the gamma distribution'), burnin=typer.Option(0.1, '--burnin', '-b', help='Fraction of trees to discard as burn-in'))</code>","text":"<p>Calculates the odds and log differences for a given set of data.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>List[Path]</code> <p>The paths to the log CSV files.</p> <code>typer.Argument(..., help='The path to the log CSV file')</code> <code>output_file</code> <code>str</code> <p>The path to the output CSV file where the results will be saved.</p> <code>typer.Argument(..., help='The path to the output CSV file where the results will be saved')</code> <code>gamma_shape</code> <code>float</code> <p>The shape parameter for the gamma distribution.</p> <code>typer.Option(..., help='The shape parameter for the gamma distribution')</code> <code>gamma_scale</code> <code>float</code> <p>The scale parameter for the gamma distribution.</p> <code>typer.Option(..., help='The scale parameter for the gamma distribution')</code> <code>burnin</code> <code>float</code> <p>Fraction of trees to discard as burn-in.</p> <code>typer.Option(0.1, '--burnin', '-b', help='Fraction of trees to discard as burn-in')</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculate_odds(['log1.csv', 'log2.csv'], 'results.csv', 2, 1, 0.1)\nCalculated odds and log differences saved to results.csv\n</code></pre> Source code in <code>src/episodic/workflow/scripts/calculate_odds.py</code> <pre><code>@app.command()\ndef calculate_odds(\n    logs: List[Path] = typer.Argument(..., help=\"The path to the log CSV file\"),\n    output_file: str = typer.Argument(..., help=\"The path to the output CSV file where the results will be saved\"),\n    gamma_shape: float = typer.Option(..., help=\"The shape parameter for the gamma distribution\"),\n    gamma_scale: float = typer.Option(..., help=\"The scale parameter for the gamma distribution\"),\n    burnin: float = typer.Option(0.1, \"--burnin\", \"-b\", help=\"Fraction of trees to discard as burn-in\"),\n):\n    \"\"\"\n    Calculates the odds and log differences for a given set of data.\n\n    Args:\n      logs (List[Path]): The paths to the log CSV files.\n      output_file (str): The path to the output CSV file where the results will be saved.\n      gamma_shape (float): The shape parameter for the gamma distribution.\n      gamma_scale (float): The scale parameter for the gamma distribution.\n      burnin (float): Fraction of trees to discard as burn-in.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; calculate_odds(['log1.csv', 'log2.csv'], 'results.csv', 2, 1, 0.1)\n      Calculated odds and log differences saved to results.csv\n    \"\"\"\n    # Read the CSV file into a DataFrame df\n    dfs = []\n    for log_path in logs:\n        duplicate = pd.read_csv(log_path, sep=\"\\t\", comment=\"#\")\n        # discard burn-in\n        dfs.append(duplicate[int(burnin * len(duplicate)) :])\n\n    df = pd.concat(dfs)\n\n    # Identify columns that end with .rate and are not 'clock.rate'\n    rate_columns = [col for col in df.columns if col.endswith(\".rate\") and col != \"clock.rate\"]\n\n    # Generate gamma-distributed random variables for p_fg and p_bg\n    n_samples = len(df)\n    p_fg = np.random.gamma(gamma_shape, gamma_scale, n_samples)\n    p_bg = np.random.gamma(gamma_shape, gamma_scale, n_samples)\n\n    # Calculate the prior odds (p_odds)\n    p_p = np.mean(p_fg &gt; p_bg)\n    p_odds = p_p / (1 - p_p) if p_p &lt; 1 else float(\"inf\")\n\n    # Results dictionary\n    results = []\n\n    # Calculate the odds for each .rate column compared to df['clock.rate']\n    for rate_column in rate_columns:\n        pos_fg = df[rate_column]\n        pos_bg = df[\"clock.rate\"]\n\n        # Calculate the posterior odds (pos_odds)\n        pos_p = np.mean(pos_fg &gt; pos_bg)\n        pos_odds = pos_p / (1 - pos_p) if pos_p &lt; 1 else float(\"inf\")\n\n        # Calculate the log difference of the odds ratios, if possible\n        log_diff = calculate_log_diff(pos_odds, p_odds)\n\n        # Store the results\n        results.append(\n            {\n                \"Rate Column\": rate_column,\n                \"p_p\": p_p,\n                \"p_odds\": p_odds,\n                \"pos_p\": pos_p,\n                \"pos_odds\": pos_odds,\n                \"bf\": log_diff,\n            }\n        )\n\n    # Create a DataFrame from the results dictionary\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(output_file, index=False)\n\n    typer.echo(f\"Calculated odds and log differences saved to {output_file}\")\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.calculate_odds.safe_log","title":"<code>safe_log(x)</code>","text":"<p>Returns the logarithm of x if x is positive, otherwise returns None.</p> Source code in <code>src/episodic/workflow/scripts/calculate_odds.py</code> <pre><code>def safe_log(x):\n    \"\"\"Returns the logarithm of x if x is positive, otherwise returns None.\"\"\"\n    if x &gt; 0:\n        return np.log(x)\n    else:\n        # Handle the case where x is not positive\n        return None\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.plot_traces.camel_to_title_case","title":"<code>camel_to_title_case(column)</code>","text":"<p>Transforms camel case to title case with space delimited.</p> <p>Regex from https://stackoverflow.com/a/9283563</p> Source code in <code>src/episodic/workflow/scripts/plot_traces.py</code> <pre><code>def camel_to_title_case(column: str):\n    \"\"\"\n    Transforms camel case to title case with space delimited.\n\n    Regex from https://stackoverflow.com/a/9283563\n    \"\"\"\n    return re.sub(r\"((?&lt;=[a-z])[A-Z]|(?&lt;!\\A)[A-Z](?=[a-z]))\", r\" \\1\", column).title()\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.plot_traces.format_fig","title":"<code>format_fig(fig)</code>","text":"<p>Formats the given figure with custom layout and style.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>plotly.graph_objects.Figure</code> <p>The figure to format.</p> required <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = go.Figure()\n&gt;&gt;&gt; format_fig(fig)\nNone\n</code></pre> Source code in <code>src/episodic/workflow/scripts/plot_traces.py</code> <pre><code>def format_fig(fig):\n    \"\"\"\n    Formats the given figure with custom layout and style.\n\n    Args:\n      fig (plotly.graph_objects.Figure): The figure to format.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; fig = go.Figure()\n      &gt;&gt;&gt; format_fig(fig)\n      None\n    \"\"\"\n    fig.update_layout(\n        width=1200,\n        height=550,\n        plot_bgcolor=\"white\",\n        title_font_color=\"black\",\n        font=dict(\n            family=\"Linux Libertine Display O\",\n            size=18,\n            color=\"black\",\n        ),\n    )\n    gridcolor = \"#dddddd\"\n    fig.update_xaxes(gridcolor=gridcolor)\n    fig.update_yaxes(gridcolor=gridcolor)\n\n    fig.update_xaxes(showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n    fig.update_yaxes(showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n    fig.update_xaxes(zeroline=True, zerolinewidth=1, zerolinecolor=gridcolor)\n    fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor=gridcolor)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.plot_traces.plot_traces","title":"<code>plot_traces(trace_log=typer.Argument(..., help='The path to the trace log from beast.'), output=typer.Argument(..., help='A path to the output directory.'), burnin=0.1)</code>","text":"<p>Plots the traces from a trace log file.</p> <p>Parameters:</p> Name Type Description Default <code>trace_log</code> <code>pathlib.Path</code> <p>The path to the trace log from beast.</p> <code>typer.Argument(..., help='The path to the trace log from beast.')</code> <code>output</code> <code>pathlib.Path</code> <p>A path to the output directory.</p> <code>typer.Argument(..., help='A path to the output directory.')</code> <code>burnin</code> <code>float</code> <p>The percentage of the trace to discard as burn-in.</p> <code>0.1</code> <p>Returns:</p> Type Description <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trace_log = Path('trace.log')\n&gt;&gt;&gt; output = Path('output')\n&gt;&gt;&gt; burnin = 0.1\n&gt;&gt;&gt; plot_traces(trace_log, output, burnin)\nNone\n</code></pre> Source code in <code>src/episodic/workflow/scripts/plot_traces.py</code> <pre><code>def plot_traces(\n    trace_log: Path = typer.Argument(..., help=\"The path to the trace log from beast.\"),\n    output: Path = typer.Argument(..., help=\"A path to the output directory.\"),\n    burnin: float = 0.1,\n):\n    \"\"\"\n    Plots the traces from a trace log file.\n\n    Args:\n      trace_log (pathlib.Path): The path to the trace log from beast.\n      output (pathlib.Path): A path to the output directory.\n      burnin (float): The percentage of the trace to discard as burn-in.\n\n    Returns:\n      None.\n\n    Examples:\n      &gt;&gt;&gt; trace_log = Path('trace.log')\n      &gt;&gt;&gt; output = Path('output')\n      &gt;&gt;&gt; burnin = 0.1\n      &gt;&gt;&gt; plot_traces(trace_log, output, burnin)\n      None\n    \"\"\"\n    output.mkdir(exist_ok=True, parents=True)\n\n    df = pd.read_csv(trace_log, sep=\"\\t\", comment=\"#\")\n    burnin_df = df.truncate(after=burnin * len(df))\n    posterior_df = df.truncate(before=burnin * len(df))\n    posterior_color = \"#1A32E6\"\n    posterior_background = \"#DDE1FA\"\n    for variable in df.columns[1:]:\n        print(variable)\n        variable_title = camel_to_title_case(variable)\n        print(f\"Plotting {variable_title}\")\n        fig = make_subplots(\n            rows=1, cols=3, subplot_titles=(\"Burn-in\", \"Posterior\", \"Histogram\"), column_widths=[0.2, 0.6, 0.2]\n        )\n        posterior_max = posterior_df[variable].max()\n        posterior_min = posterior_df[variable].min()\n        y_range_min = posterior_min - 0.1 * np.abs(posterior_max - posterior_min)\n        y_range_max = posterior_max + 0.1 * np.abs(posterior_max - posterior_min)\n        fig.add_shape(\n            type=\"rect\",\n            xref=\"x domain\",\n            yref=\"y domain\",\n            x0=0,\n            y0=0,\n            x1=1,\n            y1=1,\n            fillcolor=\"red\",\n            col=1,\n            row=1,\n            layer=\"below\",\n            opacity=0.25,\n        )\n        fig.add_shape(\n            type=\"rect\",\n            xref=\"x domain\",\n            x0=0,\n            y0=y_range_min,\n            x1=1,\n            y1=y_range_max,\n            fillcolor=posterior_background,\n            col=1,\n            row=1,\n            line={\"width\": 0},\n            layer=\"below\",\n        )\n        # fig.add_shape(\n        #     type=\"rect\",\n        #     xref=\"x domain\", yref=\"y domain\",\n        #     x0=0, y0=0,\n        #     x1=1, y1=1.0,\n        #     fillcolor=posterior_background,\n        #     col=2, row=1,\n        #     line={'width':0},\n        #     layer=\"below\",\n        # )\n        # fig.add_shape(\n        #     type=\"rect\",\n        #     xref=\"x domain\", yref=\"y domain\",\n        #     x0=0, y0=0,\n        #     x1=1, y1=1.0,\n        #     fillcolor=posterior_background,\n        #     col=3, row=1,\n        #     line={'width':0},\n        #     layer=\"below\",\n        # )\n        fig.add_trace(\n            go.Scatter(\n                x=burnin_df[\"state\"],\n                y=burnin_df[variable],\n                marker_color=\"red\",\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n            go.Scatter(x=posterior_df[\"state\"], y=posterior_df[variable], marker_color=posterior_color),\n            row=1,\n            col=2,\n        )\n        fig.add_trace(\n            go.Histogram(\n                y=posterior_df[variable], marker_color=posterior_color, histnorm=\"probability density\", opacity=0.5\n            ),\n            row=1,\n            col=3,\n        )\n        mean = posterior_df[variable].mean()\n        fig.add_annotation(\n            x=1,\n            y=mean,\n            xref=\"x domain\",\n            yref=\"y\",\n            text=f\"mean: {mean:.2g}\",\n            showarrow=False,\n            align=\"right\",\n            xanchor=\"right\",\n            yanchor=\"bottom\",\n            row=1,\n            col=3,\n        )\n        fig.add_shape(\n            type=\"line\",\n            x0=0,\n            y0=mean,\n            x1=1,\n            y1=mean,\n            xref=\"x domain\",\n            yref=\"y\",\n            line=dict(\n                color=\"#777777\",\n                width=1.5,\n                dash=\"dot\",\n            ),\n            row=1,\n            col=3,\n        )\n\n        format_fig(fig)\n        fig.data[0].update(mode=\"markers+lines\")\n        fig.update_layout(\n            yaxis_title=variable_title,\n            showlegend=False,\n        )\n        fig.update_layout(\n            xaxis1_range=[burnin_df[\"state\"].min(), burnin_df[\"state\"].max()],\n            yaxis2_range=[y_range_min, y_range_max],\n            yaxis3_range=[y_range_min, y_range_max],\n            xaxis1_title=\"states\",\n            xaxis2_title=\"states\",\n            xaxis3_title=\"Density\",\n        )\n        print([y_range_min, y_range_max])\n        fig.write_image(output / f\"{variable}.svg\")\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.tree_converter.get_schema","title":"<code>get_schema(path)</code>","text":"<p>Gets the schema for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The schema for the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the schema cannot be determined from the file extension.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_schema(Path('file.nexus'))\n'nexus'\n</code></pre> Source code in <code>src/episodic/workflow/scripts/tree_converter.py</code> <pre><code>def get_schema(path: Path):\n    \"\"\"\n    Gets the schema for a given file.\n\n    Args:\n      path (Path): The path to the file.\n\n    Returns:\n      str: The schema for the file.\n\n    Raises:\n      Exception: If the schema cannot be determined from the file extension.\n\n    Examples:\n      &gt;&gt;&gt; get_schema(Path('file.nexus'))\n      'nexus'\n    \"\"\"\n    if path.suffix in [\".nxs\", \".nexus\", \".treefile\"]:\n        return \"nexus\"\n    if path.suffix in [\".newick\", \".nwk\"]:\n        return \"newick\"\n    raise Exception(f\"Cannot get schema for file {path}\")\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.tree_converter.tree_converter","title":"<code>tree_converter(input=typer.Argument(..., help='The path to the tree file in newick format.'), output=typer.Argument(..., help='The path to the tree file in newick format.'), input_schema=typer.Option('', help='The input file schema. If empty then it tries to infer the schema from the file extension.'), output_schema=typer.Option('', help='The output file schema. If empty then it tries to infer the schema from the file extension.'), node_label=typer.Option('', help='Label the nodes from an annotation if present.'))</code>","text":"<p>Converts a tree file from one format to another.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Path</code> <p>The path to the tree file in newick format.</p> <code>typer.Argument(..., help='The path to the tree file in newick format.')</code> <code>output</code> <code>Path</code> <p>The path to the tree file in newick format.</p> <code>typer.Argument(..., help='The path to the tree file in newick format.')</code> <code>input_schema</code> <code>str</code> <p>The input file schema. If empty then it tries to infer the schema from the file extension.</p> <code>typer.Option('', help='The input file schema. If empty then it tries to infer the schema from the file extension.')</code> <code>output_schema</code> <code>str</code> <p>The output file schema. If empty then it tries to infer the schema from the file extension.</p> <code>typer.Option('', help='The output file schema. If empty then it tries to infer the schema from the file extension.')</code> <code>node_label</code> <code>str</code> <p>Label the nodes from an annotation if present.</p> <code>typer.Option('', help='Label the nodes from an annotation if present.')</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree_converter(Path('input.nwk'), Path('output.nexus'), input_schema='newick', output_schema='nexus', node_label='label')\n</code></pre> Source code in <code>src/episodic/workflow/scripts/tree_converter.py</code> <pre><code>def tree_converter(\n    input: Path = typer.Argument(..., help=\"The path to the tree file in newick format.\"),\n    output: Path = typer.Argument(..., help=\"The path to the tree file in newick format.\"),\n    input_schema: str = typer.Option(\n        \"\", help=\"The input file schema. If empty then it tries to infer the schema from the file extension.\"\n    ),\n    output_schema: str = typer.Option(\n        \"\", help=\"The output file schema. If empty then it tries to infer the schema from the file extension.\"\n    ),\n    node_label: str = typer.Option(\"\", help=\"Label the nodes from an annotation if present.\"),\n):\n    \"\"\"\n    Converts a tree file from one format to another.\n\n    Args:\n      input (Path): The path to the tree file in newick format.\n      output (Path): The path to the tree file in newick format.\n      input_schema (str): The input file schema. If empty then it tries to infer the schema from the file extension.\n      output_schema (str): The output file schema. If empty then it tries to infer the schema from the file extension.\n      node_label (str): Label the nodes from an annotation if present.\n\n    Returns:\n      None\n\n    Examples:\n      &gt;&gt;&gt; tree_converter(Path('input.nwk'), Path('output.nexus'), input_schema='newick', output_schema='nexus', node_label='label')\n    \"\"\"\n    if not input_schema:\n        input_schema = get_schema(input)\n    if not output_schema:\n        output_schema = get_schema(output)\n\n    input_str = input.read_text()\n\n    # Replace quote char because dendropy nexus tokenizer only uses single quotes by default\n    input_str = input_str.replace('\"', \"'\")\n\n    tree = dendropy.Tree.get(data=input_str, schema=input_schema)\n\n    if node_label:\n        for node in tree:\n            try:\n                node.label = \"%.2g\" % float(node.annotations.get_value(node_label))\n            except:\n                node.label = node.annotations.get_value(node_label)\n\n    tree.write(path=output, schema=output_schema, suppress_rooting=True)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.populate_beast_template.Log","title":"<code>Log</code>  <code>dataclass</code>","text":"<p>Dataclass representing a log file.</p> <p>Attributes:</p> Name Type Description <code>log_every</code> <code>int</code> <p>The frequency at which to log.</p> <code>file_name</code> <code>str</code> <p>The name of the log file.</p> Source code in <code>src/episodic/workflow/scripts/populate_beast_template.py</code> <pre><code>@dataclass\nclass Log:\n    \"\"\"\n    Dataclass representing a log file.\n\n    Attributes:\n      log_every (int): The frequency at which to log.\n      file_name (str): The name of the log file.\n    \"\"\"\n    log_every: int\n    file_name: str\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.populate_beast_template.MLE","title":"<code>MLE</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Log</code></p> <p>Dataclass representing a marginal likelihood estimator.</p> <p>Attributes:</p> Name Type Description <code>log_every</code> <code>int</code> <p>The frequency at which to log.</p> <code>file_name</code> <code>str</code> <p>The name of the log file.</p> <code>results_file_name</code> <code>str</code> <p>The name of the results file.</p> <code>chain_length</code> <code>int</code> <p>The length of the MCMC chain.</p> <code>path_steps</code> <code>int</code> <p>The number of path steps for the MLE.</p> Source code in <code>src/episodic/workflow/scripts/populate_beast_template.py</code> <pre><code>@dataclass\nclass MLE(Log):\n    \"\"\"\n    Dataclass representing a marginal likelihood estimator.\n\n    Attributes:\n      log_every (int): The frequency at which to log.\n      file_name (str): The name of the log file.\n      results_file_name (str): The name of the results file.\n      chain_length (int): The length of the MCMC chain.\n      path_steps (int): The number of path steps for the MLE.\n    \"\"\"\n    results_file_name: str\n    chain_length: int\n    path_steps: int\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.populate_beast_template.Taxon","title":"<code>Taxon</code>  <code>dataclass</code>","text":"<p>Dataclass representing a taxon.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The id of the taxon.</p> <code>sequence</code> <code>str</code> <p>The sequence of the taxon.</p> <code>date</code> <code>float</code> <p>The date of the taxon.</p> <code>uncertainty</code> <code>float</code> <p>The uncertainty of the taxon's date.</p> Source code in <code>src/episodic/workflow/scripts/populate_beast_template.py</code> <pre><code>@dataclass\nclass Taxon:\n    \"\"\"\n    Dataclass representing a taxon.\n\n    Attributes:\n      id (str): The id of the taxon.\n      sequence (str): The sequence of the taxon.\n      date (float): The date of the taxon.\n      uncertainty (float): The uncertainty of the taxon's date.\n    \"\"\"\n    id: str\n    sequence: str\n    date: float\n    uncertainty: float = 0.0\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.populate_beast_template.populate_beast_template","title":"<code>populate_beast_template(work_dir, name, template_path, alignment_path, groups, clock, rate_gamma_prior_shape=0.5, rate_gamma_prior_scale=0.1, chain_length=100000000, samples=10000, mle_chain_length=1000000, mle_path_steps=100, mle_log_every=10000, date_delimiter='|', date_index=-1, fixed_tree=None, *, trace=True, trees=True, mle=True)</code>","text":"<p>Populates a Beast XML template with an alignment file.</p> <p>Parameters:</p> Name Type Description Default <code>work_dir</code> <code>Path</code> <p>The path to the working directory.</p> required <code>name</code> <code>str</code> <p>The name of the output file.</p> required <code>template_path</code> <code>Path</code> <p>The path to the input Beast template file.</p> required <code>alignment_path</code> <code>Path</code> <p>The path to the input alignment file.</p> required <code>groups</code> <code>list</code> <p>A list of groups to include in the analysis.</p> required <code>clock</code> <code>str</code> <p>The clock model to use in the analysis.</p> required <code>rate_gamma_prior_shape</code> <code>float</code> <p>The shape parameter of the gamma prior on the rate.</p> <code>0.5</code> <code>rate_gamma_prior_scale</code> <code>float</code> <p>The scale parameter of the gamma prior on the rate.</p> <code>0.1</code> <code>chain_length</code> <code>int</code> <p>The length of the MCMC chain.</p> <code>100000000</code> <code>samples</code> <code>int</code> <p>The number of samples to draw from the MCMC chain.</p> <code>10000</code> <code>mle_chain_length</code> <code>int</code> <p>The length of the MCMC chain for the marginal likelihood estimator.</p> <code>1000000</code> <code>mle_path_steps</code> <code>int</code> <p>The number of path steps for the marginal likelihood estimator.</p> <code>100</code> <code>mle_log_every</code> <code>int</code> <p>The log every for the marginal likelihood estimator.</p> <code>10000</code> <code>date_delimiter</code> <code>str</code> <p>The delimiter for the date in the fasta header.</p> <code>'|'</code> <code>date_index</code> <code>int</code> <p>The index of the date in the fasta header.</p> <code>-1</code> <code>fixed_tree</code> <code>Path</code> <p>The path to the fixed tree file.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>trace</code> <code>bool</code> <p>Whether to enable the trace log.</p> <code>trees</code> <code>bool</code> <p>Whether to enable the trees log.</p> <code>mle</code> <code>bool</code> <p>Whether to run the marginal likelihood estimator.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The rendered Beast XML template.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; populate_beast_template(\n...     work_dir=Path(\"output\"),\n...     name=\"my_analysis\",\n...     template_path=Path(\"template.xml\"),\n...     alignment_path=Path(\"alignment.fasta\"),\n...     groups=[\"group1\", \"group2\"],\n...     clock=\"strict\",\n...     rate_gamma_prior_shape=0.5,\n...     rate_gamma_prior_scale=0.1,\n...     chain_length=100000000,\n...     samples=10000,\n...     mle_chain_length=1000000,\n...     mle_path_steps=100,\n...     mle_log_every=10000,\n...     date_delimiter=\"|\",\n...     date_index=-1,\n...     fixed_tree=Path(\"fixed_tree.nwk\"),\n...     trace=True,\n...     trees=True,\n...     mle=True,\n... )\n&lt;Rendered Beast XML template&gt;\n</code></pre> Source code in <code>src/episodic/workflow/scripts/populate_beast_template.py</code> <pre><code>def populate_beast_template(\n        work_dir: Path,\n        name: str,\n        template_path: Path,\n        alignment_path: Path,\n        groups: list,\n        clock: str,\n        rate_gamma_prior_shape: float = 0.5,\n        rate_gamma_prior_scale: float = 0.1,\n        chain_length: int = 100000000,\n        samples: int = 10000,\n        mle_chain_length: int = 1000000,\n        mle_path_steps: int = 100,\n        mle_log_every: int = 10000,\n        date_delimiter=\"|\",\n        date_index=-1,\n        fixed_tree: Optional[Path] = None,\n        *, # Force the user to specify the following arguments by name\n        trace: bool = True,\n        trees: bool = True,\n        mle: bool = True,\n    ):\n    \"\"\"\n    Populates a Beast XML template with an alignment file.\n\n    Args:\n      work_dir (Path): The path to the working directory.\n      name (str): The name of the output file.\n      template_path (Path): The path to the input Beast template file.\n      alignment_path (Path): The path to the input alignment file.\n      groups (list): A list of groups to include in the analysis.\n      clock (str): The clock model to use in the analysis.\n      rate_gamma_prior_shape (float): The shape parameter of the gamma prior on the rate.\n      rate_gamma_prior_scale (float): The scale parameter of the gamma prior on the rate.\n      chain_length (int): The length of the MCMC chain.\n      samples (int): The number of samples to draw from the MCMC chain.\n      mle_chain_length (int): The length of the MCMC chain for the marginal likelihood estimator.\n      mle_path_steps (int): The number of path steps for the marginal likelihood estimator.\n      mle_log_every (int): The log every for the marginal likelihood estimator.\n      date_delimiter (str): The delimiter for the date in the fasta header.\n      date_index (int): The index of the date in the fasta header.\n      fixed_tree (Path): The path to the fixed tree file.\n\n    Keyword Args:\n      trace (bool): Whether to enable the trace log.\n      trees (bool): Whether to enable the trees log.\n      mle (bool): Whether to run the marginal likelihood estimator.\n\n    Returns:\n      str: The rendered Beast XML template.\n\n    Examples:\n      &gt;&gt;&gt; populate_beast_template(\n      ...     work_dir=Path(\"output\"),\n      ...     name=\"my_analysis\",\n      ...     template_path=Path(\"template.xml\"),\n      ...     alignment_path=Path(\"alignment.fasta\"),\n      ...     groups=[\"group1\", \"group2\"],\n      ...     clock=\"strict\",\n      ...     rate_gamma_prior_shape=0.5,\n      ...     rate_gamma_prior_scale=0.1,\n      ...     chain_length=100000000,\n      ...     samples=10000,\n      ...     mle_chain_length=1000000,\n      ...     mle_path_steps=100,\n      ...     mle_log_every=10000,\n      ...     date_delimiter=\"|\",\n      ...     date_index=-1,\n      ...     fixed_tree=Path(\"fixed_tree.nwk\"),\n      ...     trace=True,\n      ...     trees=True,\n      ...     mle=True,\n      ... )\n      &lt;Rendered Beast XML template&gt;\n    \"\"\"\n    # Load the template\n    template = Template(template_path.read_text(), undefined=StrictUndefined)\n\n    # Parse the alignment file into Taxon objects\n    taxa = taxa_from_fasta(alignment_path, date_delimiter=date_delimiter, date_index=date_index)\n\n    if fixed_tree is not None:\n        fixed_tree = fixed_tree.read_text()\n\n    log_every = max(1, chain_length // samples)\n\n    trace_log = None\n    if trace:\n        trace_log = Log(\n            log_every=log_every,\n            file_name=f\"{name}.log\",\n        )\n\n    tree_log = None\n    if trees:\n        tree_log = Log(\n            log_every=log_every,\n            file_name=f\"{name}.trees\",\n        )\n\n    mle_log = None\n    if mle:\n        mle_log = MLE(\n            log_every = mle_log_every,\n            file_name=f\"{name}.mle.log\",\n            results_file_name=work_dir / f\"{name}.mle.results.log\",\n            chain_length=mle_chain_length,\n            path_steps=mle_path_steps,\n        )\n\n    # Render the template\n    rendered_template = template.render(\n        taxa=taxa,\n        groups=groups,\n        clock=clock,\n        fixedTree=fixed_tree,\n        rateGammaPriorShape=rate_gamma_prior_shape,\n        rateGammaPriorScale=rate_gamma_prior_scale,\n        chainLength=chain_length,\n        screenLogEvery=log_every,\n        traceLog=trace_log,\n        treeLog=tree_log,\n        marginalLikelihoodEstimator=mle_log,\n    )\n\n    # Write the rendered template to a file\n    return(rendered_template)\n</code></pre>"},{"location":"reference/scripts/#src.episodic.workflow.scripts.populate_beast_template.taxa_from_fasta","title":"<code>taxa_from_fasta(fasta_path, date_delimiter='|', date_index=-1)</code>","text":"<p>Parses a fasta file into a list of Taxon objects.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_path</code> <code>Path</code> <p>The path to the fasta file.</p> required <code>date_delimiter</code> <code>str</code> <p>The delimiter for the date in the fasta header.</p> <code>'|'</code> <code>date_index</code> <code>int</code> <p>The index of the date in the fasta header.</p> <code>-1</code> <p>Returns:</p> Type Description <code>List[Taxon]</code> <p>List[Taxon]: A list of Taxon objects representing the taxa in the fasta file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the fasta file is invalid.</p> Source code in <code>src/episodic/workflow/scripts/populate_beast_template.py</code> <pre><code>def taxa_from_fasta(fasta_path, date_delimiter=\"|\", date_index=-1) -&gt; List[Taxon]:\n    \"\"\"\n    Parses a fasta file into a list of Taxon objects.\n\n    Args:\n      fasta_path (Path): The path to the fasta file.\n      date_delimiter (str): The delimiter for the date in the fasta header.\n      date_index (int): The index of the date in the fasta header.\n\n    Returns:\n      List[Taxon]: A list of Taxon objects representing the taxa in the fasta file.\n\n    Raises:\n      ValueError: If the fasta file is invalid.\n    \"\"\"\n    # Read the fasta file\n    with open(fasta_path) as fasta_file:\n        fasta_lines = fasta_file.readlines()\n\n    # check if valid fasta\n    if not fasta_lines[0].startswith(\"&gt;\"):\n        raise ValueError(\"Invalid fasta file.\")\n    # Parse the fasta file into Taxon objects. Support multi-line sequences.\n    taxa = []\n    for line in fasta_lines:\n        if line.startswith(\"&gt;\"):\n            header = line[1:].strip()\n            # 1992/1 = 1992 to 1993\n            date_with_uncertainty = header.split(date_delimiter)[date_index]\n            date, *uncertainty = date_with_uncertainty.split(\"/\")\n            if uncertainty:\n                uncertainty = float(uncertainty[0])\n            else:\n                uncertainty = 0.0\n            try:\n                date = float(date)\n            except ValueError:\n                date = date_to_decimal_year(date)\n            taxa.append(Taxon(id=header, sequence=\"\", date=float(date), uncertainty=uncertainty))\n        else:\n            taxa[-1].sequence += line.strip()\n\n    return taxa\n</code></pre>"}]}